{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "Copy of CRISP_DM_Template (assessment 2 classification).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItWhqRu49r9_",
        "colab_type": "text"
      },
      "source": [
        "# Using the CRISP-DM Method for MLN 601 Machine Learning\n",
        "#Assessment 2: Classification \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPkAPbs99r-A",
        "colab_type": "text"
      },
      "source": [
        "Adapted from Smart Vision Europe (2020)https://www.sv-europe.com/crisp-dm-methodology/ and Patience (2018) https://grantpatience.com/2018/10/30/applying-crisp-dm-to-data-science-and-a-re-usable-template-in-jupyter/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I01Jl089r-B",
        "colab_type": "text"
      },
      "source": [
        "CRISP-DM stands for cross-industry process for data mining and represents a robust and proven framework available since 1996. While providing a structured approach to planning a data mining project is extremely useful for the documentation, building and deployment of ML projects. The model represents an ideal framework but tasks can be executed in a different sequence and if necessary repeated. The template is a cut down version for use in the completion of your assessment. Any code is shown as comments for example purposes and not necessiarily intended to be completely working. Your final assessment report should follow this template and include your code and narrative text.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NZFgj039r-C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The CRISP steps are:\n",
        "\n",
        "1. Gain an understanding of the business\n",
        "\n",
        "2. Gain an understanding of the data\n",
        "\n",
        "3. Prepare the data\n",
        "\n",
        "4. Complete modeling \n",
        "\n",
        "5. Evaluate\n",
        "\n",
        "6. Deploy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ctR3Nfz9r-D",
        "colab_type": "text"
      },
      "source": [
        "# 1. Stage One - Determine Business Objectives and Assess the Situation  <a class=\"anchor\"></a>\n",
        "The first stage of the CRISP-DM process is to understand what you want to accomplish from a business perspective. For the wine assessment think about the requirement to predict the quality of the red wine using the physicochemical traits available. Insert your understanding of the problem and what you are trying to achieve.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7RZpV__9r-E",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Assess the Current Situation<a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv2qPfF99r-G",
        "colab_type": "text"
      },
      "source": [
        "This involves more detailed fact-finding about all of the resources. Here you can specify yourself as part of the resources as well as the source of data. As far as computing resources Microsoft Azure with 4 GB of memory available. Software includes Azure Notebook and Python 3.6 for software. Don't forget any Python libraries you plan to use. \n",
        "List the resources available to the project including:\n",
        "\n",
        "- Personnel:\n",
        "- Data: \n",
        "- Computing resources: \n",
        "- Software: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtZ510Vd9r-H",
        "colab_type": "text"
      },
      "source": [
        "# 2. Stage  Two - Data Understanding <a class=\"anchor\"></a>\n",
        "The second stage of the CRISP-DM process requires you to acquire the data listed in the project resources. This initial collection includes data loading, if this is necessary for data understanding. For example, if you use a specific tool for data understanding, it makes perfect sense to load your data into this tool. If you acquire multiple data sources then you need to consider how and when you're going to integrate the various sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10LYwYq9r-H",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Initial Data Acquisition <a class=\"anchor\"></a>\n",
        "List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar projects. Ensure you are clear about the various ways in which you can import data into your Notebook. Data can ne read directly from the source e.g. Website or uploaded into the notebook from your computer or cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6RVBpQI9r-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Libraries Required\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "#import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRu3idD59r-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data source: \n",
        "#Source Query location: \n",
        "#path =  'F:/Projects/Data Science/Defaults/train_/train.csv' or URL \n",
        "# reads the data from the file - denotes as CSV, it has no header, sets column headers\n",
        "#df =  pd.read_csv(path, sep=',') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULd3-Y7r9r-X",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Describe Data <a class=\"anchor\"></a>\n",
        "Data description of the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Evaluate whether the data acquired satisfies your requirements to solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djkYjgm9r-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.columns, df.shape, df.dtypes, df.describe(), df.info() and df.head(10) Use Pandas to explore and clean up your tabular data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn48zfvE9r-a",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Verify Data Quality <a class=\"anchor\"></a>\n",
        "\n",
        "Examine the quality of the data:\n",
        "\n",
        "- Is the data complete (does it cover all that you require)?\n",
        "- Is it correct, or does the data contain errors ?\n",
        "- Are there missing values in the data? If so, where do they occur?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_xLibVB9r-b",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1. Outliers <a class=\"anchor\"></a>\n",
        "At this point, we may also want to remove any outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values or rare events. However, you would remove anomalies based on the definition of extreme outliers:\n",
        "\n",
        "https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
        "\n",
        "- Below the first quartile − 3 ∗ interquartile range\n",
        "- Above the third quartile + 3 ∗ interquartile range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5jagtIN9r-b",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Initial Data Exploration  <a class=\"anchor\"></a>\n",
        "During this stage, address data questions using querying, data visualization and reporting techniques. These may include:\n",
        "\n",
        "- **Distribution** of key attributes (for example, the target attribute of a prediction task)\n",
        "- **Relationships** between pairs or small numbers of attributes\n",
        "- Results of **simple aggregations**\n",
        "- **Properties** of significant sub-populations\n",
        "- **Simple** statistical analyses\n",
        "\n",
        "These analyses may contribute to or refine the data description and quality aspects of your report, and feed into other data preparation steps needed for further analysis. \n",
        "\n",
        "- **Data exploration component of your report** - Describe results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. Include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PuG4fY59r-c",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1 Distributions  <a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZjwoFSt9r-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_values_table(df):\n",
        "        count_val = df.value_counts()\n",
        "        count_val_percent = 100 * df.value_counts() / len(df)\n",
        "        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n",
        "        count_val_table_ren_columns = count_val_table.rename(\n",
        "        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n",
        "        return count_val_table_ren_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXORJz39r-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Histogram\n",
        "def hist_chart(df, col):\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        plt.hist(df[col].dropna(), edgecolor = 'k');\n",
        "        plt.xlabel(col); plt.ylabel('Number of Entries'); \n",
        "        plt.title('Distribution of '+col);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSnggtw9r-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# col = 'account_risk_band'\n",
        "# Histogram & Results\n",
        "# hist_chart(df, col)\n",
        "# count_values_table(df.account_risk_band)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FVZDz7I9r-m",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.2 Correlations  <a class=\"anchor\"></a>\n",
        "Can we derive any correlation from this data-set. Pairplot chart gives us correlations, distributions and regression path\n",
        "Correlogram are awesome for exploratory analysis. It allows to quickly observe the relationship between every variable of your matrix. \n",
        "It is easy to do it with seaborn: just call the pairplot function\n",
        "\n",
        "Pairplot documentation is found here: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKo-HD0m9r-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Seaborn allows to make a correlogram or correlation matrix really easily. \n",
        "#sns.pairplot(df.dropna().drop(['x'], axis=1), hue='y', kind ='reg')\n",
        "\n",
        "#plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ckxwktv9r-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_agg = df.drop(['x'], axis=1).groupby(['y']).sum()\n",
        "#df_agg = df.groupby(['y']).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTYwzdtU9r-s",
        "colab_type": "text"
      },
      "source": [
        "# 3. Stage Three - Data Preparation <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSnIzuW39r-s",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Select Your Data <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your machine learning goal, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.\n",
        "\n",
        "Rationale for inclusion/exclusion - List the data to be included/excluded and the reasons for these decisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMq8Mfe9r-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_regr = df.drop(['date_maint', 'account_open_date'], axis = 1)\n",
        "X_train = df.drop(['target', 'date_maint', 'account_open_date'], axis = 1)\n",
        "X_test = test.drop(['date_maint', 'account_open_date'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p9JwKkU9r-w",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Clean The Data <a class=\"anchor\"></a>\n",
        "This task involves raising the data quality to the level required by the analysis techniques that you've selected. This may involve selecting clean subsets of the data, the insertion of suitable defaults, or more ambitious techniques such as the estimation of missing data by modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirqThh89r-w",
        "colab_type": "text"
      },
      "source": [
        "# 4. Stage Four - Modelling <a class=\"anchor\"></a>\n",
        "As the first step in modelling, you'll select the actual modelling technique that you'll be using e.g.Decision tree\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAFdU3DZ9r-x",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Modelling technique <a class=\"anchor\"></a>\n",
        "Document the actual modelling technique that is to be used.\n",
        "\n",
        "Import Models in your code below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3oshQ_J9r-x",
        "colab_type": "text"
      },
      "source": [
        "## 4.2. Modelling assumptions <a class=\"anchor\"></a>\n",
        "Many modelling techniques make specific assumptions about the data, for example that all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any assumptions made.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBP69t3Q9r-y",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Build Model <a class=\"anchor\"></a>\n",
        "Run the modelling tool on the prepared dataset to create your model.\n",
        "\n",
        "**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n",
        "\n",
        "**Model** - This is the actual model produced by the modelling tool, not a report on the model.\n",
        "\n",
        "**Model description** - Describe the resulting model, report on the interpretation of the model and document any difficulties encountered with their meanings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WChMZ2Uk9r-y",
        "colab_type": "text"
      },
      "source": [
        "## 4.4. Assess Model <a class=\"anchor\"></a>\n",
        "Interpret the models according to your knowledge, your prediction success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically to discuss the machine learning results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n",
        "\n",
        "At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most ML projects a single technique is applied more than once and results are generated with several different techniques. \n",
        "\n",
        "**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n",
        "\n",
        "**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model. Document all such revisions and assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2xPWHz-9r-z",
        "colab_type": "text"
      },
      "source": [
        "# 5. Stage 5 - Evaluate  <a class=\"anchor\"></a>\n",
        "Previous steps deal with the accuracy and generality of the model. During this step you should assesses the degree to which the model meets your business objectives and seek to determine if there is some business reason why this model is deficient. \n",
        "\n",
        "Assessment of machine learning results - Summarise assessment results in terms of business success criteria, including a final statement regarding whether the project meets the initial business objectives.\n",
        "Approved models - After assessing models with respect to business success criteria, the generated models that meet the selected criteria become the approved models. For this initial assessment, you are only required to consider one model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4kiQur79r-z",
        "colab_type": "text"
      },
      "source": [
        "# 6. Stage 6 - Deploy  <a class=\"anchor\"></a>\n",
        "\n",
        "In the deployment stage you would determine a strategy for their deployment and document here together with ongoing monitoring and maintenance of your model. This is particularly important as a predictive machine learning model significantly impacts business operations. For the purposes of this assessment we will use this section to conclude the report. The previous steps should contain your code and narrative text inserted at the relevant sections. Here, you should look at lessons learnt. This includes the things that went right, what went wrong, what you did well and areas for improvement. Additionally, summarise any other expereinces during the project.   \n",
        "\n"
      ]
    }
  ]
}